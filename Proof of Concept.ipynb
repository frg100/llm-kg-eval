{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ff397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b21e4",
   "metadata": {},
   "source": [
    "### KnowledgeGraph framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215811ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class KnowledgeGraph(ABC):\n",
    "    def __init__(self, name=None, verbose = 0):\n",
    "        self._name = name\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._entities = [] # list(string)\n",
    "        self._relations = [] # list(string)\n",
    "        # np.array([(head_entity, relation, tail_entity)])\n",
    "        self._triples = np.zeros(shape=(0,3))\n",
    "        \n",
    "        self._built = False\n",
    "        \n",
    "    ####### PUBLIC #######\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        return self._entities\n",
    "    \n",
    "    @property\n",
    "    def relations(self):\n",
    "        return self._relations\n",
    "    \n",
    "    @property\n",
    "    def triples(self):\n",
    "        return self._triples\n",
    "    \n",
    "    def sample(self, k=1, negative=False):\n",
    "        if negative:\n",
    "            return self._sample_negative_loose(k)\n",
    "        else:\n",
    "            return self._sample_positive(k)\n",
    "        \n",
    "    def sample_english(self, k=1, negative=False):\n",
    "        samples = self.sample(k, negative)\n",
    "        \n",
    "        english_samples = []\n",
    "        for sample in samples:\n",
    "            head_idx, relation_idx, tail_idx = sample\n",
    "            head_id, relation_id, tail_id = self._entities[head_idx], self._relations[relation_idx], self._entities[tail_idx]\n",
    "            head, relation, tail = self._id2entity(head_id), self._id2relation(relation_id), self._id2entity(tail_id)\n",
    "            english_samples.append(relation.replace(\"{HEAD}\", head).replace(\"{TAIL}\", tail))\n",
    "            \n",
    "        return english_samples\n",
    "            \n",
    "        \n",
    "    ####### PRIVATE #######\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _id2entity(self, eid):\n",
    "        \"\"\"\n",
    "        A function that maps an entity id (eid) stored in the\n",
    "        self._entities structure to an english identifier\n",
    "        and/or description.\n",
    "        \"\"\"\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _id2relation(self, rid):\n",
    "        \"\"\"\n",
    "        A function that maps an relation id (rid) stored in\n",
    "        the self._relations structure to an english identifier\n",
    "        and/or description.\n",
    "        \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        A function that builds the graph by reading in the data in\n",
    "        its current format and populating self._entities, self._relations,\n",
    "        self._triples, and at the end should set self._built to True.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def _is_built(self):\n",
    "        return self._built\n",
    "    \n",
    "    @property\n",
    "    def _num_entities(self):\n",
    "        return len(self._entities)\n",
    "    \n",
    "    @property\n",
    "    def _num_relations(self):\n",
    "        return len(self._relations)\n",
    "    \n",
    "    @property\n",
    "    def _num_triples(self):\n",
    "        return self._triples.shape[0]\n",
    "    \n",
    "    def _validate_graph(self):\n",
    "        # Make sure properties are filled out\n",
    "        assert self._built, \"The graph is not built. Please build \" \\\n",
    "        \"or check that your build_graph method sets self._build \" \\\n",
    "        \"to True after completion\"\n",
    "        \n",
    "        # Make sure shape of self._triples is [N, 3]\n",
    "        assert self._triples.shape[1] == 3, \"The _triples property\" \\\n",
    "        \"must have a shape of 3 in the second dimension. \" \\\n",
    "        \n",
    "        # Make sure all head, tail entities and relations are valid\n",
    "        head_entities = self._triples[:,0]\n",
    "        assert head_entities.max() <= len(self._entities), \"There\" \\\n",
    "        \"exists an entity in the head entities of the _triples \" \\\n",
    "        \"property that exceeds the number of available entities.\" \\\n",
    "        \n",
    "        tail_entities = self._triples[:,2]\n",
    "        assert tail_entities.max() <= len(self._entities), \"There \" \\\n",
    "        \"exists an entity in the tail entities of the _triples \" \\\n",
    "        \"property that exceeds the number of available entities.\" \\\n",
    "        \n",
    "        relations = self._triples[:,1]\n",
    "        assert relations.max() <= len(self._relations), \"There \" \\\n",
    "        \"exists an relations in the _triples \" \\\n",
    "        \"property that exceeds the number of available relations.\" \\\n",
    "        \n",
    "        for eid in self._entities:\n",
    "            assert self._id2entity(eid), f\"One of the entities ({eid}) \" \\\n",
    "            \"has no mapping.\"\n",
    "            \n",
    "        for rid in self._relations:\n",
    "            assert self._id2relation(rid), f\"One of the relations ({rid}) \" \\\n",
    "            \"has no mapping.\"\n",
    "            \n",
    "        assert self.sample(10).shape == (10, 3), \"Sampling yields the \" \\\n",
    "            \"wrong shape\"\n",
    "        \n",
    "        assert self.sample(10, negative=True).shape == (10, 3), \"Sampling \" \\\n",
    "            \"yields the wrong shape\"\n",
    "        \n",
    "        if self._verbose >= 1:\n",
    "            print(\"Graph was successfully validated!\")\n",
    "        \n",
    "    def _sample_positive(self, k):\n",
    "        triple_indices = np.random.choice(self._num_triples, k)\n",
    "        positive_samples = self._triples[triple_indices]\n",
    "        \n",
    "        return positive_samples\n",
    "    \n",
    "    def _sample_negative_loose(self, k):\n",
    "        # TODO(frg100): Make a strict version that makes sure not to\n",
    "        # add existing triples\n",
    "        head_entities = np.expand_dims(np.random.choice(self._num_entities, k), 0)\n",
    "        relations = np.expand_dims(np.random.choice(self._num_relations, k), 0)\n",
    "        tail_entities = np.expand_dims(np.random.choice(self._num_entities, k), 0)\n",
    "        \n",
    "        negative_samples = np.concatenate([head_entities, relations, tail_entities], axis=0).T\n",
    "        \n",
    "        return negative_samples\n",
    "    \n",
    "    def _load_json_mapping(self, json_path):\n",
    "        # Load the map\n",
    "        with open(json_path) as json_file:\n",
    "            return json.load(json_file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae20f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB15k237(KnowledgeGraph):\n",
    "    def __init__(self, base_path=None, splits=['train', 'test', 'valid'], verbose = 0):\n",
    "        super().__init__(name='FB15k-237', verbose = verbose)\n",
    "        \n",
    "        self._base_path = base_path\n",
    "        self._splits = splits\n",
    "        \n",
    "        self._entity_mapping = None\n",
    "        \n",
    "        start = time.time()\n",
    "        self._build_graph(verbose)\n",
    "        end = time.time()\n",
    "        if verbose >= 1:\n",
    "            print(f\"Building the graph took {round(end-start)} seconds\")    \n",
    "        \n",
    "            \n",
    "    def _id2entity(self, eid):\n",
    "        if self._entity_mapping is None:\n",
    "            assert False, \"Entity mapping must be populated\"\n",
    "            \n",
    "        if eid not in self._entity_mapping:\n",
    "            #print(f\"Entity with id ({eid}) is not mapped...\")\n",
    "            return None\n",
    "            \n",
    "        return self._entity_mapping[eid]['label']\n",
    "    \n",
    "    def _id2relation(self, rid):\n",
    "        if self._relation_mapping is None:\n",
    "            assert False, \"Relation mapping must be populated\"\n",
    "            \n",
    "        if rid not in self._relation_mapping:\n",
    "            #print(f\"Relation with id ({rid}) is not mapped...\")\n",
    "            return None\n",
    "            \n",
    "        return self._relation_mapping[rid]\n",
    "\n",
    "    def _build_graph(self, verbose):\n",
    "        # Load the mappings\n",
    "        id2entity_path = os.path.join(self._base_path, \"entity2wikidata.json\")\n",
    "        self._entity_mapping = self._load_json_mapping(id2entity_path)\n",
    "        id2relation_path = os.path.join(self._base_path, \"relation_mapping.json\")\n",
    "        self._relation_mapping = self._load_json_mapping(id2relation_path)\n",
    "        \n",
    "        # Initialize data structures for bookkeeping\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        triples = set()\n",
    "\n",
    "        num_data_points = sum(sum(1 for line in open(os.path.join(self._base_path, f\"{split}.txt\"))) for split in self._splits)\n",
    "        \n",
    "        # Load data\n",
    "        for split in self._splits:\n",
    "            path = os.path.join(self._base_path, f\"{split}.txt\")\n",
    "            if verbose >= 1:\n",
    "                print(f\"Loading file {split}.txt\")\n",
    "                \n",
    "            # Process into entities, relations, and triples\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Check progress\n",
    "                    last_percent_done = round((100*(self._num_triples-1))/num_data_points)\n",
    "                    percent_done = round((100*self._num_triples)/num_data_points)\n",
    "                    if verbose >= 2 and percent_done % 5 == 0 and last_percent_done % 5 != 0:\n",
    "                        print(f\"Data loading progress: [{percent_done}%]\")\n",
    "                    \n",
    "                    # Initialize data\n",
    "                    head, relation, tail = line.split()\n",
    "                    head_id, relation_id, tail_id = None, None, None\n",
    "                    \n",
    "                    # If either of the entities has no natural language translation,\n",
    "                    if not self._id2entity(head) or not self._id2entity(tail):\n",
    "                        # Don't process it\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose >= 3 and percent_done % 5 == 0 and last_percent_done % 5 != 0:\n",
    "                        print(f\"{self._id2entity(head)} {relation} {self._id2entity(tail)}\")\n",
    "                    \n",
    "                    # Process head\n",
    "                    if head not in entities:\n",
    "                        entities.add(head)\n",
    "                        head_id = len(self._entities)\n",
    "                        self._entities.append(head)\n",
    "                    else:\n",
    "                        head_id = self._entities.index(head)\n",
    "                     \n",
    "                    # Process tail\n",
    "                    if tail not in entities:\n",
    "                        entities.add(tail)\n",
    "                        tail_id = len(self._entities)\n",
    "                        self._entities.append(tail)\n",
    "                    else:\n",
    "                        tail_id = self._entities.index(tail)\n",
    "                        \n",
    "                    # Process relation\n",
    "                    if relation not in relations:\n",
    "                        relations.add(relation)\n",
    "                        relation_id = len(self._relations)\n",
    "                        self._relations.append(relation)\n",
    "                    else:\n",
    "                        relation_id = self._relations.index(relation)\n",
    "\n",
    "                    # Create and add triple\n",
    "                    triple = np.array([[head_id, relation_id, tail_id]], dtype=np.int32)  \n",
    "                    if self._num_triples == 0:\n",
    "                        self._triples = triple\n",
    "                    else:\n",
    "                        self._triples = np.append(self._triples, triple, axis=0)\n",
    "                        \n",
    "        # Build and validate\n",
    "        self._built = True\n",
    "        self._validate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141de12",
   "metadata": {},
   "source": [
    "### Modeling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import scipy\n",
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "class LargeLanguageModel(ABC):\n",
    "    def __init__(self, name=None, verbose = 0):\n",
    "        self._name = name\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._grammar_model = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "        self._grammar_model_args = TTSettings(num_beams=5, min_length=1)\n",
    "        \n",
    "        self._built = False\n",
    "        \n",
    "    ####### PUBLIC #######\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    def correct_grammar(self, sentence):\n",
    "        return self._grammar_model.generate_text(f\"grammar: {sentence}\", args=self._grammar_model_args).text\n",
    "    \n",
    "    ####### PRIVATE #######\n",
    "    \n",
    "    @abstractmethod\n",
    "    def batch_perplexity(self, eid):\n",
    "        \"\"\"\n",
    "        A function that calculates a batch perplexity for a set of\n",
    "        samples.\n",
    "        \"\"\"\n",
    "        \n",
    "    def test_discrimatory_power(self, graph, num_examples):\n",
    "        pos_samples = graph.sample_english(num_examples)\n",
    "        neg_samples = graph.sample_english(num_examples, negative=True)\n",
    "        \n",
    "        pos_perplexity = self.batch_perplexity(pos_samples)\n",
    "        neg_perplexity = self.batch_perplexity(neg_samples)\n",
    "        \n",
    "        return scipy.stats.ttest_ind(pos_perplexity, neg_perplexity).pvalue\n",
    "        \n",
    "    @property\n",
    "    def _is_built(self):\n",
    "        return self._built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "class GPT2(LargeLanguageModel):\n",
    "    def __init__(self, model_id=\"gpt2\", verbose = 0):\n",
    "        super().__init__(name='GPT2', verbose = verbose)\n",
    "        \n",
    "        self._model_id = model_id\n",
    "        self._model = GPT2LMHeadModel.from_pretrained(self._model_id)\n",
    "        self._tokenizer = GPT2TokenizerFast.from_pretrained(self._model_id)\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._built = True\n",
    "        \n",
    "    def batch_perplexity(self, samples):\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"[{self._model_id}] Calculating perplexity for {len(samples)} samples\")\n",
    "        perplexities = []\n",
    "        for sample in tqdm(samples):\n",
    "            sample = self.correct_grammar(sample)\n",
    "            encoding = self._tokenizer(sample, return_tensors=\"pt\")\n",
    "            num_tokens = encoding.input_ids.shape[1]\n",
    "\n",
    "            nlls = []\n",
    "            for end_loc in range(1, num_tokens):\n",
    "                input_ids = encoding.input_ids[:, 0:end_loc]\n",
    "                target_ids = input_ids.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self._model(input_ids, labels=target_ids)\n",
    "                    neg_log_likelihood = outputs[0] * end_loc\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            perplexity = torch.exp(torch.stack(nlls).sum() / num_tokens)\n",
    "            if self._verbose >= 2:\n",
    "                print(f\"[{self._model_id}] Sample <{sample}> has perplexity [{perplexity}]\")\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"[{self._model_id}] Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "\n",
    "        return perplexities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BERT(LargeLanguageModel):\n",
    "    def __init__(self, name='BERT', verbose = 0):\n",
    "        super().__init__(name=name, verbose = verbose)\n",
    "        \n",
    "        self._name = name\n",
    "        \n",
    "        self._model_id = 'bert-base-cased'\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(self._model_id)\n",
    "        self._model = BertForMaskedLM.from_pretrained(self._model_id)\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._built = True\n",
    "        \n",
    "    def batch_perplexity(self, samples):\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"[{self._name}] Calculating perplexity for {len(samples)} samples\")\n",
    "        perplexities = []\n",
    "        for sample in tqdm(samples):\n",
    "            sample_english = sample\n",
    "            sample = self.correct_grammar(sample)\n",
    "            \n",
    "            encoding = self._tokenizer(sample, return_tensors=\"pt\")\n",
    "            #encoding = self._tokenizer.encode_plus(\n",
    "            #    sample,\n",
    "            #    add_special_tokens=True,\n",
    "            #    truncation = True,\n",
    "            #    padding = \"max_length\",\n",
    "            #    return_attention_mask = True,\n",
    "            #    return_tensors = \"pt\"\n",
    "            #)\n",
    "            num_tokens = encoding.input_ids.shape[1]\n",
    "            \n",
    "            input_ids = encoding.input_ids\n",
    "            target_ids = input_ids.clone()\n",
    "            \n",
    "            #print(input_ids, input_ids.shape)\n",
    "            \n",
    "            nlls = []\n",
    "            for mask_idx in range(input_ids[0].shape[0]):\n",
    "                masked_input_ids = input_ids.clone()\n",
    "                masked_input_ids[0][mask_idx] = self._tokenizer.mask_token_id\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self._model(input_ids).logits[:,mask_idx,:]\n",
    "                    softmax = F.softmax(outputs, -1)\n",
    "                    \n",
    "                    target_id = target_ids[0][mask_idx]\n",
    "                    nlls.append(softmax[0,target_id])\n",
    "\n",
    "            perplexity = torch.exp((-1/num_tokens)*torch.log(torch.stack(nlls)).sum())\n",
    "            if self._verbose >= 2:\n",
    "                print(f\"[{self._name}] Sample <{sample_english}> has perplexity [{perplexity}]\")\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"[{self._name}] Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "\n",
    "        return perplexities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_models(samples, model_a, perplexities_a, model_b, perplexities_b):\n",
    "    f, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.set(yscale=\"log\")\n",
    "\n",
    "    model_one_ppx = np.array([ppx.item() for ppx in perplexities_a])\n",
    "    model_two_ppx = np.array([ppx.item() for ppx in perplexities_b])\n",
    "\n",
    "    sort_order = np.argsort(model_one_ppx).astype(int)\n",
    "\n",
    "    sns.lineplot(x=samples, y=model_one_ppx[sort_order], label=model_a)\n",
    "    sns.lineplot(x=samples, y=model_two_ppx[sort_order], label=model_b)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title(\"Comparing perplexity on sample sentences between two models\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_triples(model, perplexities_pos, perplexities_neg):\n",
    "    f, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.set(xscale=\"log\")\n",
    "\n",
    "    pos_ppx = np.array([ppx.item() for ppx in perplexities_pos])\n",
    "    neg_ppx = np.array([ppx.item() for ppx in perplexities_neg])\n",
    "\n",
    "    sns.distplot(pos_ppx, label=\"POSITIVE\")\n",
    "    sns.distplot(neg_ppx, label=\"NEGATIVE\")\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('Perplexity Density')\n",
    "    plt.title(f\"Comparing perplexity on positive and negative sentences for {model}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8a92b",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e05b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = FB15k237(base_path='./data/FB15k-237', splits=['train', 'valid','test'], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt2 = GPT2(model_id=\"gpt2\", verbose=2)\n",
    "model_gpt2_large = GPT2(model_id=\"gpt2-large\", verbose=2)\n",
    "model_bert = BERT(verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eaecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_bert = model_bert.test_discrimatory_power(graph, 1000)\n",
    "print(f\"The power of BERT is: [{power_bert}]\")\n",
    "f = open(\"bert_results.txt\", \"w\")\n",
    "f.write(str(power_bert))\n",
    "f.close()\n",
    "\n",
    "power_gpt2 = model_gpt2.test_discrimatory_power(graph, 1000)\n",
    "print(f\"The power of gpt2 is: [{power_gpt2}]\")\n",
    "f = open(\"bert_results.txt\", \"w\")\n",
    "f.write(str(power_gpt2))\n",
    "f.close()\n",
    "\n",
    "power_gpt2_large = model_gpt2_large.test_discrimatory_power(graph, 1000)\n",
    "print(f\"The power of gpt2_large is: [{power_gpt2_large}]\")\n",
    "f = open(\"bert_results.txt\", \"w\")\n",
    "f.write(str(power_gpt2_large))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #samples = graph.sample_english(25)\n",
    "# #gpt2_pos_perplexity = model_gpt2.batch_perplexity(samples)\n",
    "# #gpt2_large_pos_perplexity = model_gpt2_large.batch_perplexity(samples)\n",
    "# bert_pos_perplexity = model_bert.batch_perplexity(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c07c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #negative_samples = graph.sample_english(25, negative=True)\n",
    "# #gpt2_neg_perplexity = model_gpt2.batch_perplexity(negative_samples)\n",
    "# #gpt2_large_neg_perplexity = model_gpt2_large.batch_perplexity(negative_samples)\n",
    "# bert_neg_perplexity = model_bert.batch_perplexity(negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "\n",
    "# print(scipy.stats.ttest_ind(gpt2_pos_perplexity, gpt2_neg_perplexity))\n",
    "# print(scipy.stats.ttest_ind(gpt2_large_pos_perplexity, gpt2_large_neg_perplexity))\n",
    "# print(scipy.stats.ttest_ind(bert_pos_perplexity, bert_neg_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a61666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.ttest_ind(bert_pos_perplexity, bert_neg_perplexity).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_models(samples, 'GPT2', gpt2_pos_perplexity, \"GPT2-Large\", gpt2_large_pos_perplexity)\n",
    "# compare_models(negative_samples, 'GPT2', gpt2_neg_perplexity, \"GPT2-Large\", gpt2_large_neg_perplexity)\n",
    "\n",
    "# compare_models(samples, 'GPT2', gpt2_pos_perplexity, \"BERT\", bert_pos_perplexity)\n",
    "# compare_models(negative_samples, 'GPT2', gpt2_neg_perplexity, \"BERT\", bert_neg_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e50bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_triples('GPT2', gpt2_pos_perplexity, gpt2_neg_perplexity)\n",
    "# compare_triples('GPT2-Large', gpt2_large_pos_perplexity, gpt2_large_neg_perplexity)\n",
    "# compare_triples('BERT', bert_pos_perplexity, bert_neg_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37a784",
   "metadata": {},
   "source": [
    "### Random Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_name = 'bert-base-cased'\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "# bert_model = BertModel.from_pretrained(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1934ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer.batch_encode_plus(\n",
    "#     graph.sample_english(10),\n",
    "#     add_special_tokens=True,\n",
    "#     return_attention_mask=True,\n",
    "#     padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be303246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_perplexity(samples, verbose=0):\n",
    "#     print(f\"Calculating perplexity for {len(samples)} samples\")\n",
    "#     perplexities = []\n",
    "#     for sample in samples:\n",
    "#         encoding = tokenizer(sample, return_tensors=\"pt\")\n",
    "#         num_tokens = encoding.input_ids.shape[1]\n",
    "\n",
    "#         nlls = []\n",
    "#         for end_loc in range(1, num_tokens):\n",
    "#             input_ids = encoding.input_ids[:, 0:end_loc]\n",
    "#             target_ids = input_ids.clone()\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(input_ids, labels=target_ids)\n",
    "#                 neg_log_likelihood = outputs[0] * end_loc\n",
    "\n",
    "#         nlls.append(neg_log_likelihood)\n",
    "\n",
    "#         perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "#         if verbose >= 2:\n",
    "#             print(f\"Sample <{sample}> has perplexity [{perplexity}]\")\n",
    "#         perplexities.append(perplexity)\n",
    "\n",
    "#     if verbose >= 1:\n",
    "#         print(f\"Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "        \n",
    "#     return perplexities\n",
    "\n",
    "# batch_perplexity(graph.sample_english(10, negative=False), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43243be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "# model_id = \"gpt2\"\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# encodings = tokenizer(\"\\n\\n\".join(graph.sample_english(10)), return_tensors=\"pt\")\n",
    "\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# max_length = model.config.n_positions\n",
    "# stride = 1\n",
    "\n",
    "# nlls = []\n",
    "# for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "#     begin_loc = max(i + stride - max_length, 0)\n",
    "#     end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "#     trg_len = end_loc - i  # may be different from stride on last loop\n",
    "#     input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "#     target_ids = input_ids.clone()\n",
    "#     target_ids[:, :-trg_len] = -100\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         print(begin_loc, end_loc)\n",
    "#         outputs = model(input_ids, labels=target_ids)\n",
    "#         neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "#     nlls.append(neg_log_likelihood)\n",
    "\n",
    "# ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08911b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e209d",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = FB15k237(base_path='./data/FB15k-237', splits=['train', 'valid','test'], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.sample_english(10, negative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00dc69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ebdcf9",
   "metadata": {},
   "source": [
    "### Random Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path='./data/FB15k-237'\n",
    "\n",
    "# mapping = {}\n",
    "\n",
    "# # Load file if it exists\n",
    "# json_path = os.path.join(base_path, \"relation_mapping.json\")\n",
    "\n",
    "# if os.path.exists(json_path):\n",
    "#     json_file_read = open(json_path, 'r')\n",
    "#     mapping = json.load(json_file_read)\n",
    "#     json_file_read.close()\n",
    "\n",
    "\n",
    "# # for rel in graph.relations:\n",
    "# #     if rel not in mapping:\n",
    "# #         relations_done = len(mapping.keys())\n",
    "# #         relations_total = len(graph.relations)\n",
    "# #         relations_left = relations_total - relations_done\n",
    "# #         print(f\"[{round(100*(relations_done/relations_total), 2)}%] done describing relations ({relations_left} left)\")\n",
    "        \n",
    "# #         instance_idx = np.where(graph.triples[:, 1] == graph.relations.index(rel))[0][0]\n",
    "# #         head, relation, tail = graph.triples[instance_idx]\n",
    "# #         head, tail = graph._mid2entity(graph.entities[head]), graph._mid2entity(graph.entities[tail])\n",
    "        \n",
    "# #         format_string = input(f\"{head} {rel} {tail}: \")\n",
    "# #         mapping[rel] = format_string\n",
    "# #         json_file_write = open(json_path, 'w')\n",
    "# #         json.dump(mapping, json_file_write)\n",
    "# #         json_file_write.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26837578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset using the functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def see_relation_examples(relation, k = 1):\n",
    "#     instance_indices = np.where(graph.triples[:, 1] == graph.relations.index(rel))[0][:k]\n",
    "#     samples = graph.triples[instance_indices]\n",
    "#     for sample in samples:\n",
    "#         h, r, t = sample\n",
    "#         print(graph._id2entity(graph.entities[h]), graph.relations[r], graph._id2entity(graph.entities[t]))\n",
    "    \n",
    "# see_relation_examples('/award/award_winner/awards_won./award/award_honor/award_winner', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(graph.relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9c82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224u",
   "language": "python",
   "name": "cs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
