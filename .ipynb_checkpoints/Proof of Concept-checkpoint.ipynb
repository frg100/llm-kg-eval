{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "875ff397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5f814",
   "metadata": {},
   "source": [
    "### KnowledgeGraph framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "215811ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class KnowledgeGraph(ABC):\n",
    "    def __init__(self, name=None, verbose = 0):\n",
    "        self._name = name\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._entities = [] # list(string)\n",
    "        self._relations = [] # list(string)\n",
    "        # np.array([(head_entity, relation, tail_entity)])\n",
    "        self._triples = np.zeros(shape=(0,3))\n",
    "        \n",
    "        self._built = False\n",
    "        \n",
    "    ####### PUBLIC #######\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        return self._entities\n",
    "    \n",
    "    @property\n",
    "    def relations(self):\n",
    "        return self._relations\n",
    "    \n",
    "    @property\n",
    "    def triples(self):\n",
    "        return self._triples\n",
    "    \n",
    "    def sample(self, k=1, negative=False):\n",
    "        if negative:\n",
    "            return self._sample_negative_loose(k)\n",
    "        else:\n",
    "            return self._sample_positive(k)\n",
    "        \n",
    "    def sample_english(self, k=1, negative=False):\n",
    "        samples = self.sample(k, negative)\n",
    "        \n",
    "        english_samples = []\n",
    "        for sample in samples:\n",
    "            head_idx, relation_idx, tail_idx = sample\n",
    "            head_id, relation_id, tail_id = self._entities[head_idx], self._relations[relation_idx], self._entities[tail_idx]\n",
    "            head, relation, tail = self._id2entity(head_id), self._id2relation(relation_id), self._id2entity(tail_id)\n",
    "            english_samples.append(relation.replace(\"{HEAD}\", head).replace(\"{TAIL}\", tail))\n",
    "            \n",
    "        return english_samples\n",
    "            \n",
    "        \n",
    "    ####### PRIVATE #######\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _id2entity(self, eid):\n",
    "        \"\"\"\n",
    "        A function that maps an entity id (eid) stored in the\n",
    "        self._entities structure to an english identifier\n",
    "        and/or description.\n",
    "        \"\"\"\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _id2relation(self, rid):\n",
    "        \"\"\"\n",
    "        A function that maps an relation id (rid) stored in\n",
    "        the self._relations structure to an english identifier\n",
    "        and/or description.\n",
    "        \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        A function that builds the graph by reading in the data in\n",
    "        its current format and populating self._entities, self._relations,\n",
    "        self._triples, and at the end should set self._built to True.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def _is_built(self):\n",
    "        return self._built\n",
    "    \n",
    "    @property\n",
    "    def _num_entities(self):\n",
    "        return len(self._entities)\n",
    "    \n",
    "    @property\n",
    "    def _num_relations(self):\n",
    "        return len(self._relations)\n",
    "    \n",
    "    @property\n",
    "    def _num_triples(self):\n",
    "        return self._triples.shape[0]\n",
    "    \n",
    "    def _validate_graph(self):\n",
    "        # Make sure properties are filled out\n",
    "        assert self._built, \"The graph is not built. Please build \" \\\n",
    "        \"or check that your build_graph method sets self._build \" \\\n",
    "        \"to True after completion\"\n",
    "        \n",
    "        # Make sure shape of self._triples is [N, 3]\n",
    "        assert self._triples.shape[1] == 3, \"The _triples property\" \\\n",
    "        \"must have a shape of 3 in the second dimension. \" \\\n",
    "        \n",
    "        # Make sure all head, tail entities and relations are valid\n",
    "        head_entities = self._triples[:,0]\n",
    "        assert head_entities.max() <= len(self._entities), \"There\" \\\n",
    "        \"exists an entity in the head entities of the _triples \" \\\n",
    "        \"property that exceeds the number of available entities.\" \\\n",
    "        \n",
    "        tail_entities = self._triples[:,2]\n",
    "        assert tail_entities.max() <= len(self._entities), \"There \" \\\n",
    "        \"exists an entity in the tail entities of the _triples \" \\\n",
    "        \"property that exceeds the number of available entities.\" \\\n",
    "        \n",
    "        relations = self._triples[:,1]\n",
    "        assert relations.max() <= len(self._relations), \"There \" \\\n",
    "        \"exists an relations in the _triples \" \\\n",
    "        \"property that exceeds the number of available relations.\" \\\n",
    "        \n",
    "        for eid in self._entities:\n",
    "            assert self._id2entity(eid), f\"One of the entities ({eid}) \" \\\n",
    "            \"has no mapping.\"\n",
    "            \n",
    "        for rid in self._relations:\n",
    "            assert self._id2relation(rid), f\"One of the relations ({rid}) \" \\\n",
    "            \"has no mapping.\"\n",
    "            \n",
    "        assert self.sample(10).shape == (10, 3), \"Sampling yields the \" \\\n",
    "            \"wrong shape\"\n",
    "        \n",
    "        assert self.sample(10, negative=True).shape == (10, 3), \"Sampling \" \\\n",
    "            \"yields the wrong shape\"\n",
    "        \n",
    "        if self._verbose >= 1:\n",
    "            print(\"Graph was successfully validated!\")\n",
    "        \n",
    "    def _sample_positive(self, k):\n",
    "        triple_indices = np.random.choice(self._num_triples, k)\n",
    "        positive_samples = self._triples[triple_indices]\n",
    "        \n",
    "        return positive_samples\n",
    "    \n",
    "    def _sample_negative_loose(self, k):\n",
    "        # TODO(frg100): Make a strict version that makes sure not to\n",
    "        # add existing triples\n",
    "        head_entities = np.expand_dims(np.random.choice(self._num_entities, k), 0)\n",
    "        relations = np.expand_dims(np.random.choice(self._num_relations, k), 0)\n",
    "        tail_entities = np.expand_dims(np.random.choice(self._num_entities, k), 0)\n",
    "        \n",
    "        negative_samples = np.concatenate([head_entities, relations, tail_entities], axis=0).T\n",
    "        \n",
    "        return negative_samples\n",
    "    \n",
    "    def _load_json_mapping(self, json_path):\n",
    "        # Load the map\n",
    "        with open(json_path) as json_file:\n",
    "            return json.load(json_file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae20f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB15k237(KnowledgeGraph):\n",
    "    def __init__(self, base_path=None, splits=['train', 'test', 'valid'], verbose = 0):\n",
    "        super().__init__(name='FB15k-237', verbose = verbose)\n",
    "        \n",
    "        self._base_path = base_path\n",
    "        self._splits = splits\n",
    "        \n",
    "        self._entity_mapping = None\n",
    "        \n",
    "        start = time.time()\n",
    "        self._build_graph(verbose)\n",
    "        end = time.time()\n",
    "        if verbose >= 1:\n",
    "            print(f\"Building the graph took {round(end-start)} seconds\")    \n",
    "        \n",
    "            \n",
    "    def _id2entity(self, eid):\n",
    "        if self._entity_mapping is None:\n",
    "            assert False, \"Entity mapping must be populated\"\n",
    "            \n",
    "        if eid not in self._entity_mapping:\n",
    "            #print(f\"Entity with id ({eid}) is not mapped...\")\n",
    "            return None\n",
    "            \n",
    "        return self._entity_mapping[eid]['label']\n",
    "    \n",
    "    def _id2relation(self, rid):\n",
    "        if self._relation_mapping is None:\n",
    "            assert False, \"Relation mapping must be populated\"\n",
    "            \n",
    "        if rid not in self._relation_mapping:\n",
    "            #print(f\"Relation with id ({rid}) is not mapped...\")\n",
    "            return None\n",
    "            \n",
    "        return self._relation_mapping[rid]\n",
    "\n",
    "    def _build_graph(self, verbose):\n",
    "        # Load the mappings\n",
    "        id2entity_path = os.path.join(self._base_path, \"entity2wikidata.json\")\n",
    "        self._entity_mapping = self._load_json_mapping(id2entity_path)\n",
    "        id2relation_path = os.path.join(self._base_path, \"relation_mapping.json\")\n",
    "        self._relation_mapping = self._load_json_mapping(id2relation_path)\n",
    "        \n",
    "        # Initialize data structures for bookkeeping\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        triples = set()\n",
    "\n",
    "        num_data_points = sum(sum(1 for line in open(os.path.join(self._base_path, f\"{split}.txt\"))) for split in self._splits)\n",
    "        \n",
    "        # Load data\n",
    "        for split in self._splits:\n",
    "            path = os.path.join(self._base_path, f\"{split}.txt\")\n",
    "            if verbose >= 1:\n",
    "                print(f\"Loading file {split}.txt\")\n",
    "                \n",
    "            # Process into entities, relations, and triples\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Check progress\n",
    "                    last_percent_done = round((100*(self._num_triples-1))/num_data_points)\n",
    "                    percent_done = round((100*self._num_triples)/num_data_points)\n",
    "                    if verbose >= 2 and percent_done % 5 == 0 and last_percent_done % 5 != 0:\n",
    "                        print(f\"Data loading progress: [{percent_done}%]\")\n",
    "                    \n",
    "                    # Initialize data\n",
    "                    head, relation, tail = line.split()\n",
    "                    head_id, relation_id, tail_id = None, None, None\n",
    "                    \n",
    "                    # If either of the entities has no natural language translation,\n",
    "                    if not self._id2entity(head) or not self._id2entity(tail):\n",
    "                        # Don't process it\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose >= 3 and percent_done % 5 == 0 and last_percent_done % 5 != 0:\n",
    "                        print(f\"{self._id2entity(head)} {relation} {self._id2entity(tail)}\")\n",
    "                    \n",
    "                    # Process head\n",
    "                    if head not in entities:\n",
    "                        entities.add(head)\n",
    "                        head_id = len(self._entities)\n",
    "                        self._entities.append(head)\n",
    "                    else:\n",
    "                        head_id = self._entities.index(head)\n",
    "                     \n",
    "                    # Process tail\n",
    "                    if tail not in entities:\n",
    "                        entities.add(tail)\n",
    "                        tail_id = len(self._entities)\n",
    "                        self._entities.append(tail)\n",
    "                    else:\n",
    "                        tail_id = self._entities.index(tail)\n",
    "                        \n",
    "                    # Process relation\n",
    "                    if relation not in relations:\n",
    "                        relations.add(relation)\n",
    "                        relation_id = len(self._relations)\n",
    "                        self._relations.append(relation)\n",
    "                    else:\n",
    "                        relation_id = self._relations.index(relation)\n",
    "\n",
    "                    # Create and add triple\n",
    "                    triple = np.array([[head_id, relation_id, tail_id]], dtype=np.int32)  \n",
    "                    if self._num_triples == 0:\n",
    "                        self._triples = triple\n",
    "                    else:\n",
    "                        self._triples = np.append(self._triples, triple, axis=0)\n",
    "                        \n",
    "        # Build and validate\n",
    "        self._built = True\n",
    "        self._validate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8b345",
   "metadata": {},
   "source": [
    "### Modeling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bdc55b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class LargeLanguageModel(ABC):\n",
    "    def __init__(self, name=None, verbose = 0):\n",
    "        self._name = name\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._built = False\n",
    "        \n",
    "    ####### PUBLIC #######\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    ####### PRIVATE #######\n",
    "    \n",
    "    @abstractmethod\n",
    "    def batch_perplexity(self, eid):\n",
    "        \"\"\"\n",
    "        A function that calculates a batch perplexity for a set of\n",
    "        samples.\n",
    "        \"\"\"\n",
    "        \n",
    "    @property\n",
    "    def _is_built(self):\n",
    "        return self._built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e3b8312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "class GPT2(LargeLanguageModel):\n",
    "    def __init__(self, verbose = 0):\n",
    "        super().__init__(name='GPT2', verbose = verbose)\n",
    "        \n",
    "        self._model_id = \"gpt2\"\n",
    "        self._model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "        self._tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        \n",
    "    def batch_perplexity(self, samples):\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"Calculating perplexity for {len(samples)} samples\")\n",
    "        perplexities = []\n",
    "        for sample in samples:\n",
    "            encoding = self._tokenizer(sample, return_tensors=\"pt\")\n",
    "            num_tokens = encoding.input_ids.shape[1]\n",
    "\n",
    "            nlls = []\n",
    "            for end_loc in range(1, num_tokens):\n",
    "                input_ids = encoding.input_ids[:, 0:end_loc]\n",
    "                target_ids = input_ids.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self._model(input_ids, labels=target_ids)\n",
    "                    neg_log_likelihood = outputs[0] * end_loc\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "            if self._verbose >= 2:\n",
    "                print(f\"Sample <{sample}> has perplexity [{perplexity}]\")\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "\n",
    "        return perplexities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4e5ff7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "class BERT(LargeLanguageModel):\n",
    "    def __init__(self, verbose = 0):\n",
    "        super().__init__(name='BERT', verbose = verbose)\n",
    "        \n",
    "        self._model_id = 'bert-base-cased'\n",
    "        self._model = BertTokenizer.from_pretrained(self._model_id)\n",
    "        self._tokenizer = BertModel.from_pretrained(self._model_id)\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        \n",
    "    def batch_perplexity(self, samples):\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"Calculating perplexity for {len(samples)} samples\")\n",
    "        perplexities = []\n",
    "        for sample in samples:\n",
    "            encoding = self._tokenizer(sample, return_tensors=\"pt\")\n",
    "            num_tokens = encoding.input_ids.shape[1]\n",
    "\n",
    "            nlls = []\n",
    "            for end_loc in range(1, num_tokens):\n",
    "                input_ids = encoding.input_ids[:, 0:end_loc]\n",
    "                target_ids = input_ids.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self._model(input_ids, labels=target_ids)\n",
    "                    neg_log_likelihood = outputs[0] * end_loc\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "            if self._verbose >= 2:\n",
    "                print(f\"Sample <{sample}> has perplexity [{perplexity}]\")\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "        if self._verbose >= 1:\n",
    "            print(f\"Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "\n",
    "        return perplexities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b4c1897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file train.txt\n",
      "Data loading progress: [5%]\n",
      "Data loading progress: [10%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f1/6dd_f1nj4fx0wmlm_tjfn1gr0000gn/T/ipykernel_2447/979707281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFB15k237\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/FB15k-237'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_gpt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/f1/6dd_f1nj4fx0wmlm_tjfn1gr0000gn/T/ipykernel_2447/2815838692.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_path, splits, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/f1/6dd_f1nj4fx0wmlm_tjfn1gr0000gn/T/ipykernel_2447/2815838692.py\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_triples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Build and validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs224u/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4743\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4744\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4745\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph = FB15k237(base_path='./data/FB15k-237', splits=['train', 'valid','test'], verbose=2)\n",
    "model_gpt2 = GPT2(verbose=2)\n",
    "model_bert = BERT(verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb518ab3",
   "metadata": {},
   "source": [
    "### Random Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ede74a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c0c752be214700ae4e830877f4020e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899d5e6158594a889b7c990d71bc93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c99794e9574416888035a66b1c9f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78243c695b1448daa03c0ad29d2a8d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "weights_name = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "bert_model = BertModel.from_pretrained(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf3f3b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2290, 112, 188, 1390, 1110, 3618, 2067, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 16745, 1110, 1388, 1107, 1103, 1970, 1735, 2614, 102, 0, 0, 0, 0, 0, 0, 0], [101, 11028, 27238, 1108, 3639, 1111, 1103, 2127, 1698, 1111, 1798, 12440, 7617, 2574, 102, 0, 0], [101, 24432, 1110, 170, 21204, 1273, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1847, 16752, 11088, 1182, 1108, 1255, 1107, 1244, 1311, 1104, 1738, 102, 0, 0, 0, 0], [101, 1109, 3207, 6486, 1104, 2715, 1104, 18959, 5208, 3293, 1110, 19349, 1777, 102, 0, 0, 0], [101, 139, 2858, 5521, 14467, 11656, 1394, 1110, 1388, 1107, 1375, 2201, 102, 0, 0, 0, 0], [101, 1789, 1234, 1104, 18787, 1234, 6585, 2936, 9001, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1109, 2523, 1109, 140, 23156, 1179, 1107, 1103, 8726, 1108, 1308, 1107, 1699, 102, 0, 0], [101, 1109, 8645, 1698, 1111, 1798, 10018, 6429, 2574, 1108, 1549, 1120, 1103, 1949, 8645, 2763, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.batch_encode_plus(\n",
    "    graph.sample_english(10),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f4396910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating perplexity for 10 samples\n",
      "Sample <Eugene Levy is a graduate of McMaster University> has perplexity [73.97415161132812]\n",
      "Sample <Joe Morton appears in the film Terminator 2: Judgment Day> has perplexity [61.194366455078125]\n",
      "Sample <The profession of will.i.am is record producer> has perplexity [262.07037353515625]\n",
      "Sample <There is a quarterback on the roster of the Dallas Cowboys> has perplexity [38.15388488769531]\n",
      "Sample <Metro-Goldwyn-Mayer distributed the film The Living Daylights> has perplexity [84.44568634033203]\n",
      "Sample <Michael McKean won an award along with Harry Shearer> has perplexity [99.71240997314453]\n",
      "Sample <Norway is located next to Russia> has perplexity [120.02970886230469]\n",
      "Sample <Jane's Addiction's music is art rock> has perplexity [449.6371154785156]\n",
      "Sample <University of Maryland, College Park had a draft pick in the 2002 Major League Baseball draft> has perplexity [37.60438919067383]\n",
      "Sample <2010: The Year We Make Contact is a drama film> has perplexity [207.28309631347656]\n",
      "Final average perplexity: 143.41050720214844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(73.9742),\n",
       " tensor(61.1944),\n",
       " tensor(262.0704),\n",
       " tensor(38.1539),\n",
       " tensor(84.4457),\n",
       " tensor(99.7124),\n",
       " tensor(120.0297),\n",
       " tensor(449.6371),\n",
       " tensor(37.6044),\n",
       " tensor(207.2831)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_perplexity(samples, verbose=0):\n",
    "    print(f\"Calculating perplexity for {len(samples)} samples\")\n",
    "    perplexities = []\n",
    "    for sample in samples:\n",
    "        encoding = tokenizer(sample, return_tensors=\"pt\")\n",
    "        num_tokens = encoding.input_ids.shape[1]\n",
    "\n",
    "        nlls = []\n",
    "        for end_loc in range(1, num_tokens):\n",
    "            input_ids = encoding.input_ids[:, 0:end_loc]\n",
    "            target_ids = input_ids.clone()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs[0] * end_loc\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        perplexity = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        if verbose >= 2:\n",
    "            print(f\"Sample <{sample}> has perplexity [{perplexity}]\")\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(f\"Final average perplexity: {sum(perplexities)/len(perplexities)}\")\n",
    "        \n",
    "    return perplexities\n",
    "\n",
    "batch_perplexity(graph.sample_english(10, negative=False), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "999ca215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a52c1d1ed4861bdbc2e9e6a983588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d859ac92f4d94a07b4dd2b71e1a67072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a7e8ecf7224037b7b8a4b19dbe7a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                   | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▋                                          | 2/137 [00:00<00:08, 15.45it/s]\u001b[A\n",
      "  4%|█▌                                         | 5/137 [00:00<00:07, 17.70it/s]\u001b[A\n",
      "  5%|██▏                                        | 7/137 [00:00<00:08, 15.94it/s]\u001b[A\n",
      "  7%|██▊                                        | 9/137 [00:00<00:10, 12.65it/s]\u001b[A\n",
      "  8%|███▎                                      | 11/137 [00:00<00:11, 10.61it/s]\u001b[A\n",
      "  9%|███▉                                      | 13/137 [00:01<00:13,  9.19it/s]\u001b[A\n",
      " 10%|████▎                                     | 14/137 [00:01<00:14,  8.51it/s]\u001b[A\n",
      " 11%|████▌                                     | 15/137 [00:01<00:15,  7.96it/s]\u001b[A\n",
      " 12%|████▉                                     | 16/137 [00:01<00:15,  7.69it/s]\u001b[A\n",
      " 12%|█████▏                                    | 17/137 [00:01<00:16,  7.12it/s]\u001b[A\n",
      " 13%|█████▌                                    | 18/137 [00:01<00:18,  6.58it/s]\u001b[A\n",
      " 14%|█████▊                                    | 19/137 [00:02<00:18,  6.31it/s]\u001b[A\n",
      " 15%|██████▏                                   | 20/137 [00:02<00:19,  6.11it/s]\u001b[A\n",
      " 15%|██████▍                                   | 21/137 [00:02<00:20,  5.73it/s]\u001b[A\n",
      " 16%|██████▋                                   | 22/137 [00:02<00:21,  5.42it/s]\u001b[A\n",
      " 17%|███████                                   | 23/137 [00:02<00:21,  5.24it/s]\u001b[A\n",
      " 18%|███████▎                                  | 24/137 [00:03<00:21,  5.22it/s]\u001b[A\n",
      " 18%|███████▋                                  | 25/137 [00:03<00:22,  4.99it/s]\u001b[A\n",
      " 19%|███████▉                                  | 26/137 [00:03<00:22,  4.85it/s]\u001b[A\n",
      " 20%|████████▎                                 | 27/137 [00:03<00:23,  4.69it/s]\u001b[A\n",
      " 20%|████████▌                                 | 28/137 [00:04<00:23,  4.61it/s]\u001b[A\n",
      " 21%|████████▉                                 | 29/137 [00:04<00:24,  4.40it/s]\u001b[A\n",
      " 22%|█████████▏                                | 30/137 [00:04<00:25,  4.24it/s]\u001b[A\n",
      " 23%|█████████▌                                | 31/137 [00:04<00:25,  4.13it/s]\u001b[A\n",
      " 23%|█████████▊                                | 32/137 [00:05<00:25,  4.12it/s]\u001b[A\n",
      " 24%|██████████                                | 33/137 [00:05<00:26,  3.97it/s]\u001b[A\n",
      " 25%|██████████▍                               | 34/137 [00:05<00:26,  3.89it/s]\u001b[A\n",
      " 26%|██████████▋                               | 35/137 [00:05<00:26,  3.83it/s]\u001b[A\n",
      " 26%|███████████                               | 36/137 [00:06<00:26,  3.79it/s]\u001b[A\n",
      " 27%|███████████▎                              | 37/137 [00:06<00:27,  3.65it/s]\u001b[A\n",
      " 28%|███████████▋                              | 38/137 [00:06<00:27,  3.56it/s]\u001b[A\n",
      " 28%|███████████▉                              | 39/137 [00:07<00:28,  3.45it/s]\u001b[A\n",
      " 29%|████████████▎                             | 40/137 [00:07<00:28,  3.45it/s]\u001b[A\n",
      " 30%|████████████▌                             | 41/137 [00:07<00:28,  3.35it/s]\u001b[A\n",
      " 31%|████████████▉                             | 42/137 [00:07<00:28,  3.29it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 43/137 [00:08<00:29,  3.22it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 44/137 [00:08<00:29,  3.17it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 45/137 [00:08<00:30,  3.06it/s]\u001b[A\n",
      " 34%|██████████████                            | 46/137 [00:09<00:30,  2.99it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 47/137 [00:09<00:30,  2.93it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 48/137 [00:10<00:30,  2.92it/s]\u001b[A\n",
      " 36%|███████████████                           | 49/137 [00:10<00:30,  2.84it/s]\u001b[A\n",
      " 36%|███████████████▎                          | 50/137 [00:10<00:31,  2.79it/s]\u001b[A\n",
      " 37%|███████████████▋                          | 51/137 [00:11<00:31,  2.74it/s]\u001b[A\n",
      " 38%|███████████████▉                          | 52/137 [00:11<00:31,  2.71it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 53/137 [00:11<00:32,  2.62it/s]\u001b[A\n",
      " 39%|████████████████▌                         | 54/137 [00:12<00:32,  2.56it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 55/137 [00:12<00:32,  2.52it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 56/137 [00:13<00:32,  2.51it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 57/137 [00:13<00:32,  2.46it/s]\u001b[A\n",
      " 42%|█████████████████▊                        | 58/137 [00:14<00:32,  2.43it/s]\u001b[A\n",
      " 43%|██████████████████                        | 59/137 [00:14<00:32,  2.39it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 60/137 [00:14<00:32,  2.37it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 61/137 [00:15<00:33,  2.30it/s]\u001b[A\n",
      " 45%|███████████████████                       | 62/137 [00:15<00:33,  2.26it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 63/137 [00:16<00:33,  2.23it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 64/137 [00:16<00:32,  2.23it/s]\u001b[A\n",
      " 47%|███████████████████▉                      | 65/137 [00:17<00:32,  2.18it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 66/137 [00:17<00:32,  2.16it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 67/137 [00:18<00:32,  2.13it/s]\u001b[A\n",
      " 50%|████████████████████▊                     | 68/137 [00:18<00:32,  2.11it/s]\u001b[A\n",
      " 50%|█████████████████████▏                    | 69/137 [00:19<00:32,  2.07it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 70/137 [00:19<00:33,  2.03it/s]\u001b[A\n",
      " 52%|█████████████████████▊                    | 71/137 [00:20<00:32,  2.01it/s]\u001b[A\n",
      " 53%|██████████████████████                    | 72/137 [00:20<00:32,  2.01it/s]\u001b[A\n",
      " 53%|██████████████████████▍                   | 73/137 [00:21<00:32,  1.98it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 74/137 [00:21<00:32,  1.96it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 75/137 [00:22<00:31,  1.94it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 76/137 [00:22<00:31,  1.93it/s]\u001b[A\n",
      " 56%|███████████████████████▌                  | 77/137 [00:23<00:31,  1.88it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 78/137 [00:23<00:31,  1.85it/s]\u001b[A\n",
      " 58%|████████████████████████▏                 | 79/137 [00:24<00:31,  1.82it/s]\u001b[A\n",
      " 58%|████████████████████████▌                 | 80/137 [00:25<00:31,  1.82it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 81/137 [00:25<00:31,  1.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▏                | 82/137 [00:26<00:31,  1.77it/s]\u001b[A\n",
      " 61%|█████████████████████████▍                | 83/137 [00:26<00:30,  1.76it/s]\u001b[A\n",
      " 61%|█████████████████████████▊                | 84/137 [00:27<00:30,  1.75it/s]\u001b[A\n",
      " 62%|██████████████████████████                | 85/137 [00:27<00:30,  1.71it/s]\u001b[A\n",
      " 63%|██████████████████████████▎               | 86/137 [00:28<00:30,  1.69it/s]\u001b[A\n",
      " 64%|██████████████████████████▋               | 87/137 [00:29<00:29,  1.67it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 88/137 [00:29<00:29,  1.67it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 89/137 [00:30<00:29,  1.65it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 90/137 [00:31<00:28,  1.63it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 91/137 [00:31<00:28,  1.61it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 92/137 [00:32<00:28,  1.60it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 93/137 [00:32<00:27,  1.57it/s]\u001b[A\n",
      " 69%|████████████████████████████▊             | 94/137 [00:33<00:28,  1.52it/s]\u001b[A\n",
      " 69%|█████████████████████████████             | 95/137 [00:34<00:27,  1.51it/s]\u001b[A\n",
      " 70%|█████████████████████████████▍            | 96/137 [00:34<00:26,  1.52it/s]\u001b[A\n",
      " 71%|█████████████████████████████▋            | 97/137 [00:35<00:26,  1.51it/s]\u001b[A\n",
      " 72%|██████████████████████████████            | 98/137 [00:36<00:26,  1.49it/s]\u001b[A\n",
      " 72%|██████████████████████████████▎           | 99/137 [00:37<00:25,  1.49it/s]\u001b[A\n",
      " 73%|█████████████████████████████▉           | 100/137 [00:37<00:24,  1.48it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 101/137 [00:38<00:24,  1.46it/s]\u001b[A\n",
      " 74%|██████████████████████████████▌          | 102/137 [00:39<00:24,  1.44it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 103/137 [00:39<00:23,  1.43it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████          | 104/137 [00:40<00:23,  1.42it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 105/137 [00:41<00:22,  1.40it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 106/137 [00:42<00:22,  1.39it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 107/137 [00:42<00:21,  1.38it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 108/137 [00:43<00:21,  1.36it/s]\u001b[A\n",
      " 80%|████████████████████████████████▌        | 109/137 [00:44<00:20,  1.34it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 110/137 [00:45<00:20,  1.32it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▏       | 111/137 [00:45<00:19,  1.30it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▌       | 112/137 [00:46<00:19,  1.31it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▊       | 113/137 [00:47<00:18,  1.30it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 114/137 [00:48<00:17,  1.29it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▍      | 115/137 [00:48<00:17,  1.29it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 116/137 [00:49<00:16,  1.28it/s]\u001b[A\n",
      " 85%|███████████████████████████████████      | 117/137 [00:50<00:15,  1.26it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 118/137 [00:51<00:15,  1.25it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 119/137 [00:52<00:14,  1.25it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 120/137 [00:53<00:13,  1.24it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 121/137 [00:53<00:13,  1.23it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 122/137 [00:54<00:12,  1.20it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 123/137 [00:55<00:11,  1.20it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████    | 124/137 [00:56<00:10,  1.20it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 125/137 [00:57<00:10,  1.18it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▋   | 126/137 [00:58<00:09,  1.17it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 127/137 [00:59<00:08,  1.16it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▎  | 128/137 [00:59<00:07,  1.16it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 129/137 [01:00<00:06,  1.14it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 130/137 [01:01<00:06,  1.13it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 131/137 [01:02<00:05,  1.13it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 132/137 [01:03<00:04,  1.12it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 133/137 [01:04<00:03,  1.10it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 134/137 [01:05<00:02,  1.10it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▍| 135/137 [01:06<00:01,  1.09it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 136/137 [01:07<00:00,  1.09it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 137/137 [01:08<00:00,  2.01it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "encodings = tokenizer(\"\\n\\n\".join(graph.sample_english(10)), return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 1\n",
    "\n",
    "nlls = []\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "    trg_len = end_loc - i  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(begin_loc, end_loc)\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b10127c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a35b7e",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37ce7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file train.txt\n",
      "Data loading progress: [5%]\n",
      "Data loading progress: [10%]\n",
      "Data loading progress: [15%]\n",
      "Data loading progress: [20%]\n",
      "Data loading progress: [25%]\n",
      "Data loading progress: [30%]\n",
      "Data loading progress: [35%]\n",
      "Data loading progress: [40%]\n",
      "Data loading progress: [45%]\n",
      "Data loading progress: [50%]\n",
      "Data loading progress: [55%]\n",
      "Data loading progress: [60%]\n",
      "Data loading progress: [65%]\n",
      "Data loading progress: [70%]\n",
      "Data loading progress: [75%]\n",
      "Data loading progress: [80%]\n",
      "Data loading progress: [85%]\n",
      "Loading file valid.txt\n",
      "Data loading progress: [90%]\n",
      "Loading file test.txt\n",
      "Data loading progress: [95%]\n",
      "Graph was successfully validated!\n",
      "Building the graph took 56 seconds\n"
     ]
    }
   ],
   "source": [
    "graph = FB15k237(base_path='./data/FB15k-237', splits=['train', 'valid','test'], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ed7270c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The currency of Frederick County is the United States dollar',\n",
       " 'Forrest Gump was nominated for the Academy Award for Best Visual Effects award',\n",
       " 'Phil Ramone has been nominated for an award along with Billy Joel',\n",
       " 'Rich Sommer won an award along with Mark Moses',\n",
       " 'Tara Reid was nominated for the Golden Raspberry Award for Worst Supporting Actress award',\n",
       " 'Dilwale Dulhania Le Jayenge is a drama film',\n",
       " 'The Gay Divorcee was nominated for the Academy Award for Best Production Design award',\n",
       " 'J. G. Ballard was influenced by William S. Burroughs',\n",
       " 'Rick Kline was nominated for an award for Terms of Endearment',\n",
       " 'The Blues Brothers is a musical film']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.sample_english(10, negative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da769255",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1224d318",
   "metadata": {},
   "source": [
    "### Random Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b7e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path='./data/FB15k-237'\n",
    "\n",
    "# mapping = {}\n",
    "\n",
    "# # Load file if it exists\n",
    "# json_path = os.path.join(base_path, \"relation_mapping.json\")\n",
    "\n",
    "# if os.path.exists(json_path):\n",
    "#     json_file_read = open(json_path, 'r')\n",
    "#     mapping = json.load(json_file_read)\n",
    "#     json_file_read.close()\n",
    "\n",
    "\n",
    "# # for rel in graph.relations:\n",
    "# #     if rel not in mapping:\n",
    "# #         relations_done = len(mapping.keys())\n",
    "# #         relations_total = len(graph.relations)\n",
    "# #         relations_left = relations_total - relations_done\n",
    "# #         print(f\"[{round(100*(relations_done/relations_total), 2)}%] done describing relations ({relations_left} left)\")\n",
    "        \n",
    "# #         instance_idx = np.where(graph.triples[:, 1] == graph.relations.index(rel))[0][0]\n",
    "# #         head, relation, tail = graph.triples[instance_idx]\n",
    "# #         head, tail = graph._mid2entity(graph.entities[head]), graph._mid2entity(graph.entities[tail])\n",
    "        \n",
    "# #         format_string = input(f\"{head} {rel} {tail}: \")\n",
    "# #         mapping[rel] = format_string\n",
    "# #         json_file_write = open(json_path, 'w')\n",
    "# #         json.dump(mapping, json_file_write)\n",
    "# #         json_file_write.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bfd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset using the functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "033a6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michelle Rodriguez /award/award_winner/awards_won./award/award_honor/award_winner Naveen Andrews\n",
      "Scott Rudin /award/award_winner/awards_won./award/award_honor/award_winner Alan Bennett\n",
      "Don Cheadle /award/award_winner/awards_won./award/award_honor/award_winner Larenz Tate\n",
      "Freddy Rodriguez /award/award_winner/awards_won./award/award_honor/award_winner Justina Machado\n",
      "Vincent Pastore /award/award_winner/awards_won./award/award_honor/award_winner Michael Imperioli\n"
     ]
    }
   ],
   "source": [
    "# def see_relation_examples(relation, k = 1):\n",
    "#     instance_indices = np.where(graph.triples[:, 1] == graph.relations.index(rel))[0][:k]\n",
    "#     samples = graph.triples[instance_indices]\n",
    "#     for sample in samples:\n",
    "#         h, r, t = sample\n",
    "#         print(graph._id2entity(graph.entities[h]), graph.relations[r], graph._id2entity(graph.entities[t]))\n",
    "    \n",
    "# see_relation_examples('/award/award_winner/awards_won./award/award_honor/award_winner', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bf2f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b58c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224u",
   "language": "python",
   "name": "cs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
